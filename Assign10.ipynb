{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b14c9-83aa-4d25-ba5f-4c5611c0ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text from my paragraph above\n",
    "text = \"\"\"I'm fascinated by Natural Language Processing (NLP), a field where computer science meets linguistics to unlock meaning from human language. NLP enables machines to understand, interpret, and generate language in ways that bridge the gap between human communication and computational analysis. What excites me most is how NLP reveals hidden patterns in language that we intuitively follow but rarely consciously notice, like the statistical distributions of words or the hierarchical structure of sentences. Beyond academic interest, I love how NLP applications have transformed our daily lives through tools like machine translation, voice assistants, and sentiment analysis systems that can process emotions in text. Perhaps most compelling is how NLP continues to advance our understanding of language itself, challenging linguists to reconsider fundamental aspects of how we communicate.\"\"\"\n",
    "\n",
    "# 1. Convert text to lowercase and remove punctuation\n",
    "text_lower = text.lower()\n",
    "text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "print(\"1. Text after lowercase and punctuation removal:\")\n",
    "print(text_no_punct[:150] + \"...\\n\")\n",
    "\n",
    "# 2. Tokenize the text into words and sentences\n",
    "sentences = sent_tokenize(text)\n",
    "words_nltk = word_tokenize(text_lower)\n",
    "\n",
    "print(\"2. Sentence tokenization:\")\n",
    "for i, sent in enumerate(sentences[:2], 1):  # Print first 2 sentences\n",
    "    print(f\"   Sentence {i}: {sent}\")\n",
    "print(f\"   Total sentences: {len(sentences)}\\n\")\n",
    "\n",
    "print(\"   Word tokenization (first 15 words):\")\n",
    "print(f\"   {words_nltk[:15]}\")\n",
    "print(f\"   Total words: {len(words_nltk)}\\n\")\n",
    "\n",
    "# 3. Compare Python split() and NLTK's word_tokenize()\n",
    "words_split = text_lower.split()\n",
    "print(\"3. Comparison between split() and word_tokenize():\")\n",
    "print(f\"   Python split() (first 15): {words_split[:15]}\")\n",
    "print(f\"   NLTK word_tokenize() (first 15): {words_nltk[:15]}\")\n",
    "\n",
    "# Find differences\n",
    "diff_examples = []\n",
    "for i in range(min(len(words_split), len(words_nltk))):\n",
    "    if i < len(words_split) and i < len(words_nltk):\n",
    "        if words_split[i] != words_nltk[i]:\n",
    "            diff_examples.append((words_split[i], words_nltk[i]))\n",
    "    if len(diff_examples) >= 3:\n",
    "        break\n",
    "\n",
    "print(\"   Key differences (split vs tokenize):\")\n",
    "for split_word, nltk_word in diff_examples:\n",
    "    print(f\"   - '{split_word}' vs '{nltk_word}'\")\n",
    "print()\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_without_stopwords = [word for word in words_nltk if word.lower() not in stop_words]\n",
    "\n",
    "print(\"4. After stopwords removal:\")\n",
    "print(f\"   Original word count: {len(words_nltk)}\")\n",
    "print(f\"   Words without stopwords: {len(words_without_stopwords)}\")\n",
    "print(f\"   First 15 words without stopwords: {words_without_stopwords[:15]}\\n\")\n",
    "\n",
    "# 5. Word frequency distribution (excluding stopwords)\n",
    "word_freq = Counter(words_without_stopwords)\n",
    "print(\"5. Word frequency distribution (top 15, excluding stopwords):\")\n",
    "for word, freq in word_freq.most_common(15):\n",
    "    print(f\"   {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f753f69-78c9-425b-93c7-3fb305528fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet') # Download wordnet\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "text = \"\"\"I'm fascinated by Natural Language Processing (NLP), a field where computer science meets linguistics to unlock meaning from human language. NLP enables machines to understand, interpret, and generate language in ways that bridge the gap between human communication and computational analysis. What excites me most is how NLP reveals hidden patterns in language that we intuitively follow but rarely consciously notice, like the statistical distributions of words or the hierarchical structure of sentences. Beyond academic interest, I love how NLP applications have transformed our daily lives through tools like machine translation, voice assistants, and sentiment analysis systems that can process emotions in text. Perhaps most compelling is how NLP continues to advance our understanding of language itself, challenging linguists to reconsider fundamental aspects of how we communicate.\"\"\"\n",
    "\n",
    "# 1. Extract all words with only alphabets using re.findall()\n",
    "words_alpha_only = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "print(\"1. Words with only alphabets:\")\n",
    "print(f\"   Count: {len(words_alpha_only)}\")\n",
    "print(f\"   First 15 words: {words_alpha_only[:15]}\\n\")\n",
    "\n",
    "# 2. Remove stop words using NLTK's stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_without_stopwords = [word for word in words_alpha_only if word.lower() not in stop_words]\n",
    "print(\"2. After stopwords removal:\")\n",
    "print(f\"   Count: {len(words_without_stopwords)}\")\n",
    "print(f\"   First 15 words: {words_without_stopwords[:15]}\\n\")\n",
    "\n",
    "# 3. Perform stemming with PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word.lower()) for word in words_without_stopwords]\n",
    "print(\"3. Stemmed words:\")\n",
    "print(f\"   First 15 stemmed words: {stemmed_words[:15]}\\n\")\n",
    "\n",
    "# 4. Perform lemmatization with WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words_without_stopwords]\n",
    "print(\"4. Lemmatized words:\")\n",
    "print(f\"   First 15 lemmatized words: {lemmatized_words[:15]}\\n\")\n",
    "\n",
    "# 5. Compare stemmed and lemmatized outputs\n",
    "print(\"5. Comparison between stemming and lemmatization:\")\n",
    "comparison = []\n",
    "for original, stemmed, lemmatized in zip(words_without_stopwords[:15], stemmed_words[:15], lemmatized_words[:15]):\n",
    "    if stemmed != lemmatized:\n",
    "        comparison.append((original, stemmed, lemmatized))\n",
    "\n",
    "print(\"   Word                 | Stemmed              | Lemmatized\")\n",
    "print(\"   ---------------------|--------------------- |-----------------------\")\n",
    "for original, stemmed, lemmatized in comparison:\n",
    "    print(f\"   {original.ljust(20)} | {stemmed.ljust(20)} | {lemmatized}\")\n",
    "\n",
    "# Show additional examples that demonstrate key differences\n",
    "interesting_examples = [\n",
    "    \"running\", \"studies\", \"better\", \"goes\", \"languages\", \"programming\", \"computational\",\n",
    "    \"machines\", \"analysis\", \"bodies\", \"mice\", \"geese\", \"children\", \"processing\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional examples highlighting differences:\")\n",
    "print(\"   Word                 | Stemmed              | Lemmatized\")\n",
    "print(\"   ---------------------|--------------------- |-----------------------\")\n",
    "\n",
    "for word in interesting_examples:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    if stemmed != lemmatized:\n",
    "        print(f\"   {word.ljust(20)} | {stemmed.ljust(20)} | {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247693ff-f5c6-4949-aa06-99f54f3d23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 1. Create three short texts (different in style and content)\n",
    "text1 = \"New smartphone features revolutionary AI camera system that adapts to various lighting conditions.\"\n",
    "text2 = \"Restaurant offers authentic Italian cuisine with homemade pasta and imported ingredients from Sicily.\"\n",
    "text3 = \"Climate scientists warn that global temperatures could rise by 2 degrees within the next decade.\"\n",
    "\n",
    "# Put all texts in a list\n",
    "texts = [text1, text2, text3]\n",
    "print(\"Text 1:\", text1)\n",
    "print(\"Text 2:\", text2)\n",
    "print(\"Text 3:\", text3)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. Generate Bag of Words representation using CountVectorizer\n",
    "print(\"PART 1: BAG OF WORDS REPRESENTATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the texts\n",
    "count_matrix = count_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Get feature names (words)\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary (all unique words across all texts)\n",
    "print(f\"Vocabulary (unique words): {len(count_feature_names)}\")\n",
    "print(f\"Vocabulary: {count_feature_names}\\n\")\n",
    "\n",
    "# Print the bag of words representation for each text\n",
    "print(\"Bag of Words representation:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\nText {i+1} counts:\")\n",
    "    # Get the indices of non-zero elements\n",
    "    non_zero_indices = count_matrix[i].nonzero()[1]\n",
    "    # Get the counts for those indices\n",
    "    counts = count_matrix[i].toarray()[0][non_zero_indices]\n",
    "    # Sort by count (descending)\n",
    "    sorted_indices = np.argsort(-counts)\n",
    "\n",
    "    # Print word-count pairs\n",
    "    for idx in sorted_indices:\n",
    "        word_idx = non_zero_indices[idx]\n",
    "        word = count_feature_names[word_idx]\n",
    "        count = counts[idx]\n",
    "        print(f\"  - '{word}': {int(count)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. Generate TF-IDF representation\n",
    "print(\"PART 2: TF-IDF REPRESENTATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Get feature names (words)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF scores for each text and identify top keywords\n",
    "print(\"TF-IDF scores and top keywords:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\nText {i+1} TF-IDF scores:\")\n",
    "\n",
    "    # Get the indices of non-zero elements\n",
    "    non_zero_indices = tfidf_matrix[i].nonzero()[1]\n",
    "    # Get the TF-IDF scores for those indices\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0][non_zero_indices]\n",
    "    # Sort by TF-IDF score (descending)\n",
    "    sorted_indices = np.argsort(-tfidf_scores)\n",
    "\n",
    "    # Print all word-score pairs\n",
    "    for idx in sorted_indices:\n",
    "        word_idx = non_zero_indices[idx]\n",
    "        word = tfidf_feature_names[word_idx]\n",
    "        score = tfidf_scores[idx]\n",
    "        print(f\"  - '{word}': {score:.4f}\")\n",
    "\n",
    "    # Print the top 3 keywords\n",
    "    top_indices = sorted_indices[:3]\n",
    "    top_words = [tfidf_feature_names[non_zero_indices[idx]] for idx in top_indices]\n",
    "    print(f\"\\n  Top 3 keywords: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Interpretation of results\n",
    "print(\"PART 3: INTERPRETATION OF TOP KEYWORDS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Get top 3 keywords for each text\n",
    "text1_keywords = []\n",
    "text2_keywords = []\n",
    "text3_keywords = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Get the indices of non-zero elements\n",
    "    non_zero_indices = tfidf_matrix[i].nonzero()[1]\n",
    "    # Get the TF-IDF scores for those indices\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0][non_zero_indices]\n",
    "    # Sort by TF-IDF score (descending)\n",
    "    sorted_indices = np.argsort(-tfidf_scores)\n",
    "    # Get top 3 keywords\n",
    "    top_indices = sorted_indices[:3]\n",
    "    top_words = [tfidf_feature_names[non_zero_indices[idx]] for idx in top_indices]\n",
    "\n",
    "    if i == 0:\n",
    "        text1_keywords = top_words\n",
    "    elif i == 1:\n",
    "        text2_keywords = top_words\n",
    "    else:\n",
    "        text3_keywords = top_words\n",
    "\n",
    "print(f\"Text 1 Top Keywords: {', '.join(text1_keywords)}\")\n",
    "print(\"Interpretation: These keywords correctly identify the main subject (AI camera system on a smartphone) and\")\n",
    "print(\"highlight that it's about new technology with adaptive capabilities.\\n\")\n",
    "\n",
    "print(f\"Text 2 Top Keywords: {', '.join(text2_keywords)}\")\n",
    "print(\"Interpretation: These keywords effectively capture the essence of an Italian restaurant with\")\n",
    "print(\"focus on authentic cuisine and ingredients from Sicily.\\n\")\n",
    "\n",
    "print(f\"Text 3 Top Keywords: {', '.join(text3_keywords)}\")\n",
    "print(\"Interpretation: These keywords accurately reflect the climate science context and the\")\n",
    "print(\"warning about temperature increase in a specific timeframe.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ba141-eaa9-4e6d-bd45-cace2dd58b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 1. Create two texts describing different technologies\n",
    "text1 = \"\"\"Artificial Intelligence mimics human cognitive functions through complex algorithms and neural networks.\n",
    "AI systems can analyze vast datasets, recognize patterns, and make decisions with minimal human intervention.\n",
    "Modern AI applications include voice assistants, recommendation systems, and autonomous vehicles.\n",
    "The field continues to evolve with advances in deep learning and natural language processing technologies.\n",
    "Concerns about AI include ethical considerations, potential job displacement, and questions of accountability.\"\"\"\n",
    "\n",
    "text2 = \"\"\"Blockchain technology creates immutable distributed ledgers that record transactions across peer networks.\n",
    "Each block contains cryptographically secure data linked to previous blocks, forming an unalterable chain.\n",
    "Beyond cryptocurrencies, blockchain enables smart contracts, decentralized finance, and supply chain tracking.\n",
    "The technology eliminates the need for central authorities through consensus mechanisms like proof-of-work.\n",
    "Challenges include scalability issues, energy consumption concerns, and regulatory uncertainty in many jurisdictions.\"\"\"\n",
    "\n",
    "print(\"Text 1 (AI):\", text1)\n",
    "print(\"\\nText 2 (Blockchain):\", text2)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. Preprocess and tokenize both texts\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Process both texts\n",
    "tokens1 = preprocess_text(text1)\n",
    "tokens2 = preprocess_text(text2)\n",
    "\n",
    "print(\"PREPROCESSING AND TOKENIZATION\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Text 1 tokens ({len(tokens1)} tokens):\")\n",
    "print(tokens1)\n",
    "print(f\"\\nText 2 tokens ({len(tokens2)} tokens):\")\n",
    "print(tokens2)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3a. Calculate Jaccard Similarity\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Convert token lists to sets\n",
    "set1 = set(tokens1)\n",
    "set2 = set(tokens2)\n",
    "\n",
    "# Calculate Jaccard similarity\n",
    "jaccard_sim = jaccard_similarity(set1, set2)\n",
    "\n",
    "# Find common terms and unique terms\n",
    "common_terms = sorted(list(set1.intersection(set2)))\n",
    "unique_to_text1 = sorted(list(set1 - set2))\n",
    "unique_to_text2 = sorted(list(set2 - set1))\n",
    "\n",
    "print(\"JACCARD SIMILARITY ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Jaccard Similarity: {jaccard_sim:.4f}\")\n",
    "print(f\"\\nCommon terms ({len(common_terms)}):\")\n",
    "print(common_terms)\n",
    "print(f\"\\nTerms unique to Text 1 (AI) ({len(unique_to_text1)}):\")\n",
    "print(unique_to_text1)\n",
    "print(f\"\\nTerms unique to Text 2 (Blockchain) ({len(unique_to_text2)}):\")\n",
    "print(unique_to_text2)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3b. Calculate Cosine Similarity using TF-IDF\n",
    "# Combine preprocessed tokens back into strings for TfidfVectorizer\n",
    "text1_processed = ' '.join(tokens1)\n",
    "text2_processed = ' '.join(tokens2)\n",
    "corpus = [text1_processed, text2_processed]\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get TF-IDF scores for each document\n",
    "text1_tfidf = tfidf_matrix[0].toarray()[0]\n",
    "text2_tfidf = tfidf_matrix[1].toarray()[0]\n",
    "\n",
    "# Create dictionaries mapping words to their TF-IDF scores\n",
    "text1_word_scores = {feature_names[i]: text1_tfidf[i] for i in range(len(feature_names)) if text1_tfidf[i] > 0}\n",
    "text2_word_scores = {feature_names[i]: text2_tfidf[i] for i in range(len(feature_names)) if text2_tfidf[i] > 0}\n",
    "\n",
    "# Sort by TF-IDF score\n",
    "text1_word_scores = dict(sorted(text1_word_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "text2_word_scores = dict(sorted(text2_word_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"COSINE SIMILARITY ANALYSIS\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF words for Text 1 (AI):\")\n",
    "for i, (word, score) in enumerate(list(text1_word_scores.items())[:10]):\n",
    "    print(f\"  {i+1}. {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF words for Text 2 (Blockchain):\")\n",
    "for i, (word, score) in enumerate(list(text2_word_scores.items())[:10]):\n",
    "    print(f\"  {i+1}. {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3c. Compare and analyze the similarity metrics\n",
    "print(\"SIMILARITY METRICS COMPARISON\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Jaccard Similarity: {jaccard_sim:.4f}\")\n",
    "print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"1. Jaccard similarity only considers word presence/absence, showing us that the two\")\n",
    "print(\"   technology descriptions share relatively few common terms (shown by the low score).\")\n",
    "print(\"\\n2. Cosine similarity accounts for word frequency and importance through TF-IDF,\")\n",
    "print(\"   providing a more nuanced measure of document similarity that weighs important terms more heavily.\")\n",
    "print(\"\\n3. The low values for both metrics confirm that AI and blockchain are distinct technologies\")\n",
    "print(\"   with largely different terminologies and conceptual frameworks.\")\n",
    "print(\"\\n4. Cosine similarity gives better insights in this case because:\")\n",
    "print(\"   - It captures the relative importance of terms specific to each technology domain\")\n",
    "print(\"   - It's less sensitive to document length differences\")\n",
    "print(\"   - It better represents the semantic distance between the specialized vocabularies\")\n",
    "print(\"   - The weighting of terms reflects their discriminative power within each technological context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c8fa4-8f77-41f9-9708-7322d21fc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create multiple product reviews for analysis\n",
    "reviews = [\n",
    "    \"The Sony WH-1000XM5 headphones offer exceptional noise cancellation and stunning sound quality. The battery life is impressive, lasting over 30 hours on a single charge. While the price is on the higher side, the comfort and audio performance completely justify the investment. The touch controls are intuitive and responsive.\",\n",
    "\n",
    "    \"This smartphone has decent performance but the camera quality is disappointing for the price point. Battery life is average at best, often requiring a mid-day charge with normal usage. The display is bright and vibrant though, which is one of its few redeeming qualities.\",\n",
    "\n",
    "    \"The air fryer works as expected. It cooks food evenly and is easy to clean. Setup was straightforward and it doesn't take up too much counter space. Nothing particularly impressive or disappointing about it.\",\n",
    "\n",
    "    \"These wireless earbuds are absolutely incredible! The sound quality rivals much more expensive models, and the noise cancellation is surprisingly effective for the compact size. The case provides multiple charges, and they're comfortable enough to wear all day. Best tech purchase I've made this year!\",\n",
    "\n",
    "    \"I regret purchasing this laptop. It constantly overheats during basic tasks, the fan is distractingly loud, and the battery depletes in under 3 hours. Customer service was unhelpful when I tried to address these issues. Save your money and look elsewhere for a reliable computer.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame to store the reviews and analysis\n",
    "df = pd.DataFrame({\n",
    "    'Review': reviews\n",
    "})\n",
    "\n",
    "print(\"PRODUCT REVIEWS DATASET\")\n",
    "print(\"-\" * 50)\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"Review {i+1}: {review}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# 1. Sentiment Analysis using TextBlob and VADER\n",
    "print(\"SENTIMENT ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize VADER analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to analyze sentiment with both TextBlob and VADER\n",
    "def analyze_sentiment(text):\n",
    "    # TextBlob analysis\n",
    "    blob = TextBlob(text)\n",
    "    textblob_polarity = blob.sentiment.polarity\n",
    "    textblob_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    # VADER analysis\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "    vader_compound = vader_scores['compound']\n",
    "\n",
    "    return textblob_polarity, textblob_subjectivity, vader_compound\n",
    "\n",
    "# Apply sentiment analysis to each review\n",
    "sentiment_results = [analyze_sentiment(review) for review in reviews]\n",
    "df['TextBlob_Polarity'] = [result[0] for result in sentiment_results]\n",
    "df['TextBlob_Subjectivity'] = [result[1] for result in sentiment_results]\n",
    "df['VADER_Compound'] = [result[2] for result in sentiment_results]\n",
    "\n",
    "# Display results\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(f\"  TextBlob Polarity: {row['TextBlob_Polarity']:.4f} (-1 to 1, negative to positive)\")\n",
    "    print(f\"  TextBlob Subjectivity: {row['TextBlob_Subjectivity']:.4f} (0 to 1, objective to subjective)\")\n",
    "    print(f\"  VADER Compound Score: {row['VADER_Compound']:.4f} (-1 to 1, negative to positive)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# 2. Classify reviews into Positive / Negative / Neutral\n",
    "print(\"SENTIMENT CLASSIFICATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Function to classify sentiment based on TextBlob and VADER\n",
    "def classify_sentiment(textblob_polarity, vader_compound):\n",
    "    # Use average of both metrics for classification\n",
    "    avg_score = (textblob_polarity + vader_compound) / 2\n",
    "\n",
    "    if avg_score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif avg_score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply classification\n",
    "df['Sentiment_Class'] = df.apply(lambda row: classify_sentiment(\n",
    "    row['TextBlob_Polarity'], row['VADER_Compound']), axis=1)\n",
    "\n",
    "# Display classification results\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Review {i+1}: {row['Sentiment_Class']}\")\n",
    "    first_few_words = \" \".join(row['Review'].split()[:5]) + \"...\"\n",
    "    print(f\"  \\\"{first_few_words}\\\"\")\n",
    "    print()\n",
    "\n",
    "# Count of each sentiment class\n",
    "sentiment_counts = df['Sentiment_Class'].value_counts()\n",
    "print(\"Sentiment Distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"  {sentiment}: {count}\")\n",
    "\n",
    "print(\"\\n=\" * 80 + \"\\n\")\n",
    "\n",
    "# 3. Create a word cloud from positive reviews\n",
    "print(\"WORD CLOUD GENERATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get all positive reviews\n",
    "positive_reviews = df[df['Sentiment_Class'] == 'Positive']['Review']\n",
    "print(f\"Number of positive reviews for word cloud: {len(positive_reviews)}\")\n",
    "\n",
    "if len(positive_reviews) > 0:\n",
    "    # Combine all positive reviews\n",
    "    positive_text = \" \".join(positive_reviews)\n",
    "\n",
    "    # Preprocess the text\n",
    "    # Convert to lowercase\n",
    "    positive_text = positive_text.lower()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(positive_text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    filtered_text = \" \".join(filtered_tokens)\n",
    "\n",
    "    print(\"\\nPreprocessed positive review text for word cloud:\")\n",
    "    print(filtered_text[:200] + \"...\" if len(filtered_text) > 200 else filtered_text)\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        contour_width=3,\n",
    "        contour_color='steelblue'\n",
    "    ).generate(filtered_text)\n",
    "\n",
    "    # Get the top 10 words in the word cloud\n",
    "    word_freq = wordcloud.process_text(filtered_text)\n",
    "    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(\"\\nTop 10 words in positive reviews:\")\n",
    "    for word, freq in top_words:\n",
    "        print(f\"  {word}: {freq}\")\n",
    "\n",
    "    print(\"\\nWord cloud generated successfully. The most prominent words represent the\")\n",
    "    print(\"most frequently mentioned positive aspects in the reviews.\")\n",
    "    print(\"\\nIf displayed, the word cloud would show words like 'quality', 'sound',\")\n",
    "    print(\"'comfortable', 'impressive', etc. in varying sizes based on their frequency,\")\n",
    "    print(\"with the most frequent words appearing largest.\")\n",
    "else:\n",
    "    print(\"No positive reviews found to generate word cloud.\")\n",
    "\n",
    "# Summary of analysis\n",
    "print(\"\\n=\" * 80 + \"\\n\")\n",
    "print(\"SUMMARY OF SENTIMENT ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"This analysis demonstrates how sentiment analysis tools like TextBlob and VADER\")\n",
    "print(\"can extract insights from product reviews. Key findings:\")\n",
    "print(\"\\n1. TextBlob provides both polarity (positive/negative) and subjectivity measures,\")\n",
    "print(\"   while VADER is specifically designed for social media and short texts.\")\n",
    "print(\"\\n2. The sentiment classification combined both approaches for more robust results.\")\n",
    "print(\"\\n3. Word clouds from positive reviews highlight the product features and qualities\")\n",
    "print(\"   that customers value most, which can be valuable for marketing and product development.\")\n",
    "print(\"\\n4. Polarity scores aligned well with the actual sentiment in the reviews, correctly\")\n",
    "print(\"   identifying positive and negative opinions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187e1f4-ea6e-416a-8499-75212a0e3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 1. Define the paragraph as training data\n",
    "paragraph = \"\"\"\n",
    "Natural language processing has transformed how we interact with technology in our daily lives.\n",
    "Voice assistants can understand complex commands and respond naturally to human queries.\n",
    "Machine translation systems break down language barriers by converting text between different languages.\n",
    "Sentiment analysis helps companies understand customer feedback at scale without manual review.\n",
    "Text summarization algorithms condense lengthy documents while preserving key information.\n",
    "Chatbots provide instant customer service by interpreting questions and generating helpful responses.\n",
    "Language models can now generate creative content like stories, poems, and even code with minimal prompting.\n",
    "The future of NLP promises even deeper understanding of context, nuance, and cultural references.\n",
    "\"\"\"\n",
    "\n",
    "# Print the training data\n",
    "print(\"TRAINING DATA:\")\n",
    "print(paragraph)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([paragraph])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"Vocabulary size: {total_words} words\")\n",
    "print(\"Sample of word-to-index mapping:\")\n",
    "# Print first 10 word-index pairs\n",
    "items = list(tokenizer.word_index.items())[:10]\n",
    "for word, idx in items:\n",
    "    print(f\"  '{word}': {idx}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. Create input sequences\n",
    "input_sequences = []\n",
    "for line in paragraph.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    if not token_list:  # Skip empty lines\n",
    "        continue\n",
    "\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
    "print(f\"Maximum sequence length: {max_sequence_length}\")\n",
    "\n",
    "# Pad sequences for consistent input dimensions\n",
    "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# Create features and labels from sequences\n",
    "X = padded_sequences[:, :-1]  # all except last word (input)\n",
    "y = padded_sequences[:, -1]   # only last word (target)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "print(\"Input shape (X):\", X.shape)\n",
    "print(\"Output shape (y):\", y.shape)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Build the model\n",
    "print(\"MODEL ARCHITECTURE:\")\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_sequence_length-1),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 5. Train the model\n",
    "print(\"TRAINING THE MODEL...\")\n",
    "history = model.fit(X, y, epochs=100, verbose=0)  # verbose=0 to suppress output\n",
    "\n",
    "# Print training results\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 6. Generate new text\n",
    "print(\"TEXT GENERATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\n",
    "        # Predict next word\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "        # Get top 3 predictions\n",
    "        top_indices = np.argsort(predicted_probs)[-3:][::-1]\n",
    "\n",
    "        # Choose one with some randomness (temperature)\n",
    "        # This helps generate more diverse text\n",
    "        predicted_index = random.choice(top_indices)\n",
    "\n",
    "        # Find the word corresponding to the predicted index\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                seed_text += \" \" + word\n",
    "                break\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "# Generate from different seed words\n",
    "seed_texts = [\"Natural language\", \"Voice assistants\", \"The future\"]\n",
    "\n",
    "for i, seed in enumerate(seed_texts):\n",
    "    print(f\"\\nSeed {i+1}: '{seed}'\")\n",
    "    generated_text = generate_text(seed, next_words=15,\n",
    "                                  model=model,\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  max_sequence_len=max_sequence_length)\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 7. Analysis and explanation\n",
    "print(\"ANALYSIS OF TEXT GENERATION\")\n",
    "print(\"-\"*50)\n",
    "print(\"The LSTM model learns patterns in the training text by establishing:\")\n",
    "print(\"- Word associations (which words tend to follow others)\")\n",
    "print(\"- Contextual relationships (how words relate in longer sequences)\")\n",
    "print(\"- Grammatical structures (basic syntax patterns)\")\n",
    "print(\"\\nLimitations of this simple model:\")\n",
    "print(\"1. The small training dataset (~100 words) limits vocabulary and pattern learning\")\n",
    "print(\"2. The model lacks broader knowledge beyond the training paragraph\")\n",
    "print(\"3. It can sometimes produce repetitive or nonsensical sequences\")\n",
    "print(\"4. It has no concept of broader context or true understanding\")\n",
    "print(\"\\nThe generated text often reflects patterns and topics from the training paragraph\")\n",
    "print(\"while attempting to form grammatically coherent sentences, though sometimes with\")\n",
    "print(\"logical inconsistencies due to the model's limited understanding.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
